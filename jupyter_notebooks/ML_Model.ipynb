{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **This notebook handle ML modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Here we create a ML model to predict diabetes based on the features engineered in the previous notebook.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* The input is the diabeters_model_input.csv file located in the data/transformed directory. This file was generated in the Feature_Engineering notebook. \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# ML model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Import the libraries needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Load the input file to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('Data/transformed/diabetes_model_input.csv')\n",
        "# display the shape and first few rows of the dataframe\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* chekcking for the valule counts of the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['diabetes'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We split the data into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate the features and target variable\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['diabetes'],axis=1),\n",
        "                                    df['diabetes'],\n",
        "                                    test_size=0.2,\n",
        "                                   #stratify=df['diabetes'],\n",
        "                                    random_state=42\n",
        "                                    )\n",
        "\n",
        "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* we create a pipeline with a scaler, feature selector and a classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pipeline_logistic_regression():\n",
        "\n",
        "    pipeline = Pipeline(\n",
        "        [\n",
        "            (\"feat_scaling\", StandardScaler()),\n",
        "            (\"feat_selection\", SelectFromModel(LogisticRegression(random_state=42))),\n",
        "            (\"model\", LogisticRegression(random_state=42)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Fit the pipeline with the train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = pipeline_logistic_regression()\n",
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Next we learn the model coefficients "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def logistic_regression_coef(model, columns):\n",
        "\n",
        "    coeff_df = pd.DataFrame(\n",
        "        model.coef_, index=[\"Coefficient\"], columns=columns\n",
        "    ).T.sort_values([\"Coefficient\"], key=abs, ascending=False)\n",
        "    print(coeff_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_regression_coef(\n",
        "    model=pipeline[\"model\"],\n",
        "    columns=X_train.columns[pipeline[\"feat_selection\"].get_support()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Here's what we observe from the coefficients\n",
        "\n",
        "| Feature              | Coefficient | Interpretation                                                              |\n",
        "|----------------------|-------------|-----------------------------------------------------------------------------|\n",
        "| HbA1c_level          | 2.45        | Strong positive impact on the probability of diabetes                       |\n",
        "| blood_glucose_level  | 1.82        | Also a strong positive impact, but slightly less than HbA1c                 |\n",
        "| age                  | 0.91        | Smaller positive impact, but still relevant                                 |\n",
        "\n",
        "\n",
        "* The model selected the 3 features based on the importance of their logistic regression coefficients.\n",
        "\n",
        "* Among them, HbA1c level is the most influential, followed by blood glucose level, and then age.\n",
        "\n",
        "* The model is interpreting higher values for all three as increasing the chance of the positive outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Now we will look at the confusion matrix and classification report for performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
        "    \n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print(\"---  Confusion Matrix  ---\")\n",
        "    print(\n",
        "        pd.DataFrame(\n",
        "            confusion_matrix(y_true=y, y_pred=prediction),\n",
        "            columns=[[\"Prediction \" + sub for sub in label_map]],\n",
        "            index=[[\"Actual \" + sub for sub in label_map]],\n",
        "        )\n",
        "    )\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"---  Classification Report  ---\")\n",
        "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
        "    \n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
        "\n",
        "    print(\"#### Test Set ####\\n\")\n",
        "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    pipeline=pipeline,\n",
        "    label_map=[\"No Diabetes\", \"Diabetes\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ What Looks Good\n",
        "\n",
        "- **Overall Accuracy is solid:**\n",
        "  - Train: 85%\n",
        "  - Test: 86%\n",
        "  - No major signs of overfitting or underfitting.\n",
        "\n",
        "- **Performance is slightly better for the 'Diabetes' class**, which is often the more critical one to catch:\n",
        "  - Test set F1-score for Diabetes: 0.87\n",
        "  - Recall for Diabetes: 0.87 (highly valuable in medical diagnosis)\n",
        "\n",
        "- **Balanced macro averages:**  \n",
        "  Precision, recall, and F1 are all around 0.85, which shows that the model doesn't heavily favor one class over another.\n",
        "\n",
        "  ## ⚠️ What Might Be Concerning\n",
        "\n",
        "- **False Negatives (missed diabetes cases):**\n",
        "  - Test: 133 missed diabetic patients\n",
        "  - That's ~13% of diabetic cases going undetected.\n",
        "  - In real-world healthcare, this could be dangerous, as undiagnosed diabetes can lead to complications.\n",
        "\n",
        "- **False Positives (wrongly predicted diabetes):**\n",
        "  - Test: 126 non-diabetic people predicted as diabetic\n",
        "  - This can cause anxiety, unnecessary testing, and cost.\n",
        "  - Even though this number is relatively low, its impact on patient well-being should not be dismissed.\n",
        "\n",
        "  ## ⚙️ Experimentation with Stratification and Class Weights\n",
        "\n",
        "I experimented with applying **stratification** and **class-weight balancing** to improve model performance, especially to reduce false negatives.\n",
        "\n",
        "- However, these changes **increased the number of false negatives**, meaning more actual diabetes cases were predicted as no diabetes.\n",
        "- Since **minimizing false negatives is critical** in this healthcare context, I chose **not to keep these changes**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ✅ Summary\n",
        "> The model demonstrates strong and balanced performance, achieving an accuracy of **85% on the training set** and **86% on the test set**. It maintains consistent precision and recall across both classes, with particularly good performance in identifying diabetes cases (**F1-score: 0.87** on the test set). The confusion matrix reveals a manageable number of false negatives and false positives, but since this is a healthcare application, further steps may be needed to validate these predictions before clinical action is taken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Implementing a Random Forest Classifier to see if it improves performance over Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Define the pipeline with a scaler, feature selector and the Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pipeline_rf_clf():\n",
        "  pipeline = Pipeline([\n",
        "      ( \"feat_scaling\",StandardScaler() ),\n",
        "      ( \"feat_selection\",SelectFromModel(RandomForestClassifier(random_state=42)) ),\n",
        "      ( \"model\", RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)),\n",
        "\n",
        "    ])\n",
        "\n",
        "  return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Using GridSearchCV to find the best hyperparameters for the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# param_grid = {\"model__n_estimators\":[50,20],\n",
        "#               }\n",
        "\n",
        "# param_grid = {\n",
        "#     'model__n_estimators': [50, 100],\n",
        "#     'model__max_depth': [None, 10, 20]\n",
        "# }\n",
        "\n",
        "# param_grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grid = GridSearchCV(estimator=pipeline_rf_clf(),\n",
        "#                     param_grid=param_grid,\n",
        "#                     cv=2,\n",
        "#                     n_jobs=-2,\n",
        "#                     verbose=1,\n",
        "#                     scoring=make_scorer(recall_score, pos_label=1)\n",
        "#                     )\n",
        "\n",
        "# grid = GridSearchCV(\n",
        "#     estimator=pipeline_rf_clf(),\n",
        "#     param_grid=param_grid,\n",
        "#     scoring=make_scorer(recall_score, pos_label=1),  # assuming 1 = 'Diabetes'\n",
        "#     cv=3,\n",
        "#     n_jobs=-1,\n",
        "#     verbose=2\n",
        "# )\n",
        "\n",
        "# grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* we check the resulst of 4 models using cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (pd.DataFrame(grid.cv_results_)\n",
        "# .sort_values(by='mean_test_score',ascending=False)\n",
        "# .filter(['params','mean_test_score'])\n",
        "# .values\n",
        "#  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* checking the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grid.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* getting the pipeline with the best estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_rf = pipeline_rf_clf()\n",
        "pipeline_rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* evaluate the model with a confusion matrix and classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_rf,\n",
        "                label_map= ['No Diabetes', 'Diabetes'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔍 Side-by-Side Comparison\n",
        "\n",
        "#### ✅ Accuracy\n",
        "\n",
        "| Model              | Train Accuracy | Test Accuracy |\n",
        "|--------------------|----------------|---------------|\n",
        "| Random Forest      | 91%            | 90%           |\n",
        "| Logistic Regression | 85%            | 86%           |\n",
        "\n",
        "> 🔹 **Winner: Random Forest** — better overall accuracy, and smaller train-test gap.\n",
        "\n",
        "---\n",
        "\n",
        "#### ✅ Recall (especially important for Diabetes class)\n",
        "\n",
        "| Model              | Train Recall – Diabetes | Test Recall – Diabetes |\n",
        "|--------------------|------------------------|-----------------------|\n",
        "| Random Forest      | 0.94                   | 0.94                  |\n",
        "| Logistic Regression | 0.86                   | 0.87                  |\n",
        "\n",
        "> 🔹 **Winner: Random Forest** — significantly higher recall for the Diabetes class, which is crucial in medical diagnosis.\n",
        "\n",
        "---\n",
        "\n",
        "#### ✅ False Negatives (missed diabetes cases)\n",
        "\n",
        "| Model              | Test False Negatives    |\n",
        "|--------------------|------------------------|\n",
        "| Random Forest      | 66 (6.5%)               |\n",
        "| Logistic Regression | 133 (13%)               |\n",
        "\n",
        "> 🔹 **Winner: Random Forest** — detects more true diabetes cases (almost halves the number of missed diagnoses).\n",
        "\n",
        "### ✅ Final Verdict\n",
        "\n",
        "Random Forest clearly outperforms Logistic Regression across all major metrics, especially in terms of recall for diabetic patients, overall accuracy, and reduction in false negatives — making it the stronger choice between the 2 models for this diabetes prediction task.\n",
        "\n",
        "## ⚙️ Experimentation with hyperparameter optimisation\n",
        "\n",
        "- Initially tried with just 'n_estimators' but the model overfitted the training data. The recall rate in train set was 1 but it was performing poorly on the test set. \n",
        "\n",
        "- So added 'max_depth' to the hyperparameter grid to control the depth of each tree in the forest.\n",
        "\n",
        "- This helped reduce overfitting and improved generalization to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
